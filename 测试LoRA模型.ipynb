{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1da4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"./data/\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed045fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.5.7: Fast Qwen2_5_Vl patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    model_name=\"lora_model\", # è®­ç»ƒçš„æ¨¡å‹\n",
    "    load_in_4bit=True,  # å¦‚æœè®¾ç½®æˆFalseå¯¹åº”çš„æ˜¯16bit LoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127e0e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The panoramic radiograph revealed diffuse periapical radiolucencies with mixed calcified contents involving multiple teeth (arrows). The periapical lesions extended to the root apices of 45 and 46 (top arrow), and of the left maxillary canine (left middle arrow), right mandibular first molar (middle arrow) and lateral incisor (right middle arrow). Additionally, it exhibited a periodontal radiolucency around the tooth 28.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "image = dataset[0][\"image\"]\n",
    "instruction = \"ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¾å°„ç§‘åŒ»ç”Ÿã€‚è¯·å‡†ç¡®æè¿°ä½ åœ¨å›¾ç‰‡ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \"content\":[\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# add_generation_prompt è®¾ç½®ä¸ºTrueç›¸å½“äºæ˜¯æ·»åŠ  assistant ä¿¡æ¯\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False, # å‰é¢å·²ç»æ˜¯åº”ç”¨äº†apply_chat_template()å·²ç»æ·»åŠ äº†è¿™äº›ç‰¹æ®Šç¬¦å·\n",
    "    return_tensors = 'pt',\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True) # skip_promptè®¾ç½®ä¸ºTrueå°±æ˜¯è¿”å›ç»“æœä¸åŒ…æ‹¬è¾“å…¥çš„æç¤ºè¯\n",
    "# min_p è®¾ç½®äº†ä¸€ä¸ªæœ€ä½å€¼ï¼Œåªæœ‰è¾¾åˆ°è¿™ä¸ªå€¼çš„tokenæ‰ä¼šè¢«è€ƒè™‘ã€‚è¿™ä¸ªå€¼ä¼šæ ¹æ®æœ€é«˜æ¦‚ç‡tokençš„ç½®ä¿¡åº¦è€Œå˜åŒ–ã€‚\n",
    "# å¦‚æœè®¾ç½®ä¸º0.1ï¼Œé‚£æ„å‘³ç€å®ƒåªä¼šå…è®¸æ¦‚ç‡è‡³å°‘æ˜¯æœ€å¤§æ¦‚ç‡tokençš„1/10ï¼Œå¦‚æœè®¾ç½®ä¸º0.05ï¼Œåˆ™ä¼šå…è®¸è‡³å°‘æ˜¯æœ€å¤§æ¦‚ç‡tokençš„1/20çš„tokenã€‚\n",
    "# temperature æ¸©åº¦æ˜¯ä¸€ä¸ªç”¨äºæ§åˆ¶AIç”Ÿæˆæ–‡æœ¬çš„åˆ›é€ åŠ›/å¤šæ ·æ€§æ°´å¹³çš„å‚æ•°ã€‚\n",
    "# æ¯ä¸€ä¸ªæ—¶åˆ»è¾“å‡ºçš„tokenså¯é€‰é¡¹ä¼šæœ‰ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ¸©åº¦è¶Šé«˜ï¼Œåˆ›é€ åŠ›å°±å˜ä½\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
